{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Effective Approaches to Attention-based Neural Machine Translation-0.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:800px\"/>\n",
    "\n",
    "*[EMNLP 2015](https://arxiv.org/abs/1508.04025)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper explores different types of attention:\n",
    "- In terms of scoping:\n",
    "  - Global attention: considering all the encoder hidden states when calculating the context vector $$c_i=\\sum_{j=1}^{T_x}{\\alpha_{ij}h_j}$$\n",
    "  - Local attention: focusing on a small window of context $[p_t-D,p_t+D]$ where $D$ is empirically selected and the aligned position $p_t$ is predicted as $$p_t=S.sigmoid(v_p^\\top tanh(W_ph_t))$$ $W_p$ and $v_p$ are model parameters, $h_t$ is encoder hidden state, $S$ is length of input sentence.\n",
    "- In terms of attention scoring between decoder hidden state $s_t$ and encoding state $h_i$:\n",
    "  - dot product: $s_t^\\top h_i$\n",
    "  - general: $s_t^\\top W_a h_i$, where $W_a$ is a trainable weight matrix\n",
    "  - concat: $v_p^\\top tanh(W_a[s_{t};h_i])$ (used in Bahdanau's paper)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
