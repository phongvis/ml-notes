* 2024, Phong Nguyen*

<div>
<p align="center">
  <img src="figure1.png" style="width:800px"/>
</p>

<a href='https://arxiv.org/abs/2005.14165'><img src='https://img.shields.io/badge/dynamic/json?url=https://api.semanticscholar.org/graph/v1/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc?fields=citationCount&query=citationCount&label=NeurIPS%202020&prefix=citation%20'/></a>

- This giant transformer GPT-3 model with 175B parameters (GPT-2 has 1.5B parameters) really brings competitive performance in few-shot settings.
- The paper terms it `in-context learning`, where a task description and a few examples are provided into a prompt and no gradient update happens. The model learns a new task through the given context.

<p align="center">
  <img src="figure2.png" style="width:800px"/>
</p>