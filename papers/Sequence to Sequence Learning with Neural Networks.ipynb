{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Sequence to Sequence Learning with Neural Networks-0.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:800px\"/>\n",
    "\n",
    "*[NeurIPS 2014](https://arxiv.org/abs/1409.3215)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the first paper that proposes an end-to-end neural network architecture for machine translation.\n",
    "- The same RNN Encoder-Decoder architecture but different from *Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation* in a few points:\n",
    "  - The RNN uses a stack of 4 LSTM layers \n",
    "  - Context vector $c$ is only used as the initial hidden state of the decoder whereas it is used as an extra input in every decoding step in the other paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Sequence to Sequence Learning with Neural Networks-1.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:800px\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
