{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation-0.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:800px\"/>\n",
    "\n",
    "*[EMNLP 2014](https://arxiv.org/abs/1406.1078)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is probably the earliest paper that applies neural networks for machine translation.\n",
    "- Proposing an RNN Encoder-Decoder architecture: learns to encode a variable-length sequence $\\textbf{x}=(x_1,x_2,\\ldots,x_T)$ into a fixed-length vector representation $c$ using an RNN and then to decode $\\textbf{c}$ back into a variable-length sequence $\\textbf{y}=(y_1,y_2,\\ldots,y_{T'})$ using another RNN.\n",
    "  - Encoder: $h_t = f(h_{t-1}, x_{t})$, then $c=h_T$\n",
    "  - Decoder: $s_t = f(s_{t-1}, y_{t-1}, c)$\n",
    "- For the choice of RNN, this paper introduces GRU (gated recurrent unit) as a comparable alternative to LSTM.\n",
    "- This is not an end-to-end machine translation model. The model is used to calculate probability of a pair of source and target phrases, which is then used as an additional feature in an SMT system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation-1.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:400px\" />"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
